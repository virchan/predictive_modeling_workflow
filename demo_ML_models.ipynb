{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7959c5b6",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Model Training</h1>\n",
    "\n",
    "<h4 style=\"text-align:center\"><a href = 'https://github.com/virchan' style='text-align:center'>https://github.com/virchan</a></h4> \n",
    "\n",
    "<h2>Abstract</h2>\n",
    "\n",
    "In the previous notebook, `demo_titanic_data_cleaning.ipynb`, we performed data cleaning on the dataset provided by the Kaggle competition \"Titanic - Machine Learning from Disaster\". Now, we proceed to train machine learning models on the refined dataset to evaluate their performance in the subsequent `demo_model_evaluation.ipynb` file.\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "\n",
    "This notebook is a continuation of the demo_titanic_data_cleaning.ipynb file, focusing on model training. Our goal is to predict the survival of Titanic passengers by training nine machine learning models on a dataset obtained from the Kaggle competition __Titanic - Machine Learning from Disaster ([Kaggle link](https://www.kaggle.com/competitions/titanic/))__.\n",
    "\n",
    "This document serves as a supplementary resource to the general report provided in the `README.md` file. It aims to provide technical details on the machine learning models used and presents a well-organized workflow of the model training stage. The outcome of this stage is the predictions made by each model, which will be further analyzed and evaluated in the `demo_model_evaluation.ipynb` file.\n",
    "\n",
    "<h2>Initiating the Models</h2>\n",
    "\n",
    "To begin our analysis, we import the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4758da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from titanic_ml_classes.titanic_machine_learning_models import titanic_machine_learning_models as titanic_models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94a13e",
   "metadata": {},
   "source": [
    "Among these libraries is `titanic_machine_learning_models`, which includes the `titanic_models` class designed to streamline our workflow. The `Titanic_ML` instance organizes our machine learning models using a Python dictionary, accessible via the `.models_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399a3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic_ML: titanic_models = titanic_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2506d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy\n",
      "tree\n",
      "forest\n",
      "support_vector\n",
      "neural_network\n",
      "logistic\n",
      "gaussian_NB\n",
      "bernoulli_NB\n",
      "adaboost\n"
     ]
    }
   ],
   "source": [
    "for name in Titanic_ML.models_.keys():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61325e0",
   "metadata": {},
   "source": [
    "The dictionary comprises a diverse range of models, including:\n",
    "\n",
    "* Two probabilistic models: Gaussian and Bernoulli Naive Bayesian Classifiers\n",
    "* Two ensemble methods: Random Forest Classifier and Adaboost Classifier\n",
    "* One base model: Decision Tree Classifier\n",
    "* One linear model: Logistic Regressor\n",
    "* One deep learning model: Feedforward Neural Network\n",
    "* One support vector machine: Support Vector Classifier\n",
    "* One baseline model: Dummy Classifier\n",
    "\n",
    "Including the Dummy Classifier is important as it provides a baseline for model comparison. If a classifier cannot outperform the Dummy Classifier, it lacks reliability. While Logistic Regression serves as the simplest non-trivial model for classification tasks, we acknowledge that linear models may not capture complex data structures related to human survivability. Therefore, we introduce the Neural Network and Support Vector Machine models, which offer increased reliability. Moving forward, we note that the features in our training set are categorical. In practical model training, it is often more convenient to convert categorical features into multiple binary features, treating them as a set of yes-or-no questions. This approach naturally aligns with the Decision Tree Classifier, Bernoulli Naive Bayes Classifier, and arguably, the Random Forest Classifier. Additionally, we include the Gaussian Naive Bayes Classifier and Adaboost Classifier to diversify our range of models.\n",
    "\n",
    "For detailed information on each model, please refer to the table below:\n",
    "\n",
    "<table style = \"width:90%\">\n",
    "    \n",
    "  <tr>\n",
    "    <th style=\"text-align: center\">Name</th>\n",
    "    <th style=\"text-align: center\">Notation</th>\n",
    "    <th style=\"text-align: center\">Type</th>\n",
    "    <th style=\"text-align: center\">Parameters</th>\n",
    "    <th style=\"text-align: center\">Documentation</th>\n",
    "    <th style=\"text-align: center\">Note</th>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td style=\"text-align: right\">Dummy Classifier</td>\n",
    "    <td style=\"text-align: center\"><code>dummy</code></td>\n",
    "    <td style=\"text-align: center\">Baseline Model</td>\n",
    "    <td style=\"text-align: center\">Default</td>\n",
    "    <td style=\"text-align: center\"><a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html'>link</a></td>\n",
    "    <td style=\"text-align: left\">Always returns the most frequent label</td>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td style=\"text-align: right\">Decision Tree Classifier</td>\n",
    "    <td style=\"text-align: center\"><code>tree</code></td>\n",
    "    <td style=\"text-align: center\">Base Model</td>\n",
    "    <td style=\"text-align: center\">Default</td>\n",
    "    <td style=\"text-align: center\"><a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html'>link</a></td>\n",
    "    <td style=\"text-align: left\"></td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td style=\"text-align: right\">Random Forest Classifier</td>\n",
    "    <td style=\"text-align: center\"><code>forest</code></td>\n",
    "    <td style=\"text-align: center\">Ensemble Method</td>\n",
    "      <td style=\"text-align: center\"><code>n_estimators = 201</code></td>\n",
    "      <td style=\"text-align: center\"><a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html'>link</a></td>\n",
    "      <td style=\"text-align: left\"></td>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td style=\"text-align: right\">Support Vector Classifier</td>\n",
    "    <td style=\"text-align: center\"><code>support_vector</code></td>\n",
    "    <td style=\"text-align: center\">Support Vector Machine</td>\n",
    "      <td style=\"text-align: center\">\n",
    "          <p><code>C = 1000</code>,</p>\n",
    "          <p><code>gamma = 0.01</code>,</p>\n",
    "          <p><code>kernel = 'rbf'</code>,</p>\n",
    "          <p><code>probability = True</code></p>\n",
    "      </td>\n",
    "    <td style=\"text-align: center\"><a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html'>link</a></td>\n",
    "    <td style=\"text-align: left\"></td>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td style=\"text-align: right\">Neural Network</td>\n",
    "    <td style=\"text-align: center\"><code>neural_network</code></td>\n",
    "    <td style=\"text-align: center\">Deep Learning Method</td>\n",
    "    <td style=\"text-align: center\">\n",
    "        <p>Five <code>Dense</code> layers with <code>units = 26, 13, 7, 4, 2</code>.</p>\n",
    "        <p>The last layer is activated by <code>softmax</code> and the rest are by <code>relu</code>.</p>\n",
    "        <p>Compiled with <code>loss = 'binary_crossentropy'</code> and <code>optimizer = 'adam'</code>.\n",
    "      </td>\n",
    "    <td style=\"text-align: center\"><a href = 'https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense'>link</a></td>\n",
    "    <td style=\"text-align: left\">Feedforward neural network</td>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td style=\"text-align: right\">Logistic Regression</td>\n",
    "    <td style=\"text-align: center\"><code>logistic</code></td>\n",
    "    <td style=\"text-align: center\">Linear Model</td>\n",
    "      <td style=\"text-align: center\"><code>solver = 'liblinear'</code></td>\n",
    "    <td style=\"text-align: center\"><a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html'>link</a></td>\n",
    "    <td style=\"text-align: left\"></td>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td style=\"text-align: right\">Gaussian Naive Bayes Classifier</td>\n",
    "    <td style=\"text-align: center\"><code>gaussian_NB</code></td>\n",
    "    <td style=\"text-align: center\">Probabilistic Model</td>\n",
    "      <td style=\"text-align: center\"><code>var_smoothing = 0.1</code></td>\n",
    "    <td style=\"text-align: center\"><a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html'>link</a></td>\n",
    "    <td style=\"text-align: left\"></td>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td style=\"text-align: right\">Bernoulli Naive Bayes Classifier</td>\n",
    "    <td style=\"text-align: center\"><code>bernoulli_NB</code></td>\n",
    "    <td style=\"text-align: center\">Probabilistic Model</td>\n",
    "      <td style=\"text-align: center\"><code>var_smoothing = 0.1</code></td>\n",
    "    <td style=\"text-align: center\"><a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB'>link</a></td>\n",
    "    <td style=\"text-align: left\"></td>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td style=\"text-align: right\">AdaBoost Classifier</td>\n",
    "    <td style=\"text-align: center\"><code>adaboost</code></td>\n",
    "    <td style=\"text-align: center\">Ensemble Method</td>\n",
    "      <td style=\"text-align: center\"><code>n_estimators = 61</code></td>\n",
    "    <td style=\"text-align: center\"><a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB'>link</a></td>\n",
    "    <td style=\"text-align: left\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<h2>Training the Models</h2>\n",
    "\n",
    "Next, we load the dataset into the `data_` dataframe and divide it into training and testing sets in a 7:3 ratio. This dataset, named `csv/train_cleaned.csv`, is a refined version of the original `csv/train.csv` file provided by Kaggle.  We carefully selected specific features from the dataset for training, including the `Pclass`, `Sex`, `Cabin`, `age_group`, and `group_size` columns. The rationale behind the selection of these features and the details of our data cleaning process can be found in the `demo_titanic_data_cleaning.ipynb` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2994edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data_: pd.DataFrame = titanic_models.transform_data(pd.read_csv('csv/train_cleaned.csv'))\n",
    "\n",
    "# Features for model training\n",
    "X: pd.DataFrame = data_.drop(['PassengerId', 'Survived'], \n",
    "                             axis = 1 \n",
    "                            )\n",
    "\n",
    "# Labels for model training\n",
    "y: pd.DataFrame = data_[['PassengerId', 'Survived']]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Store the passenger's id for each sample\n",
    "id_train: pd.DataFrame = y_train['PassengerId']\n",
    "id_test: pd.DataFrame = y_test['PassengerId']\n",
    "\n",
    "y_train: pd.DataFrame = y_train.drop('PassengerId', axis = 1).values.ravel()\n",
    "y_test: pd.DataFrame = y_test.drop('PassengerId', axis = 1).values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b5579",
   "metadata": {},
   "source": [
    "As shown, the training set consists of 623 samples, while the testing set contains 268 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5154eaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size = (623, 25)\n",
      "Testing set size = (268, 25)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size = {X_train.shape}\")\n",
    "print(f\"Testing set size = {X_test.shape}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117d1db4",
   "metadata": {},
   "source": [
    "The `.fit()` method of the `Titanic_ML` instance emulates similar methods in the `sklearn` and `tensorflow` libraries. Calling the `.fit()` method trains all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce14c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dummy...\n",
      "Model dummy is ready!\n",
      "\n",
      "Training tree...\n",
      "Model tree is ready!\n",
      "\n",
      "Training forest...\n",
      "Model forest is ready!\n",
      "\n",
      "Training support_vector...\n",
      "Model support_vector is ready!\n",
      "\n",
      "Training neural_network...\n",
      "Model neural_network is ready!\n",
      "\n",
      "Training logistic...\n",
      "Model logistic is ready!\n",
      "\n",
      "Training gaussian_NB...\n",
      "Model gaussian_NB is ready!\n",
      "\n",
      "Training bernoulli_NB...\n",
      "Model bernoulli_NB is ready!\n",
      "\n",
      "Training adaboost...\n",
      "Model adaboost is ready!\n",
      "\n",
      "All models are ready.\n"
     ]
    }
   ],
   "source": [
    "# Train the ML models\n",
    "Titanic_ML.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b64acd",
   "metadata": {},
   "source": [
    "We can examine model parameters, such as the coefficients of the `logistic` model, and view the summary of the `neural_network` model, as displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b9e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of the Logistic Regression\n",
      "[[-0.40361233 -1.34358455 -2.40555844 -0.21863331 -0.47624875  0.21383911\n",
      "  -0.11445664  0.49324893  0.66921742 -0.07984428 -0.63432954  1.53655829\n",
      "   0.20999235  0.55806973  0.62198737 -0.02018114 -0.33324737  0.15586519\n",
      "   0.42124079  0.34369067 -1.15015701 -1.04390872 -0.32121673 -0.27755068\n",
      "  -0.74975699]]\n",
      "\n",
      "\n",
      "Neural Network Summary\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 26)                676       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 13)                351       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 98        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 32        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,167\n",
      "Trainable params: 1,167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check some of the models\n",
    "\n",
    "print(\"Coefficients of the Logistic Regression\")\n",
    "print(Titanic_ML.models_['logistic'].coef_)\n",
    "print(\"\\n\")\n",
    "print(\"Neural Network Summary\")\n",
    "print(Titanic_ML.models_['neural_network'].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ac5c7",
   "metadata": {},
   "source": [
    "<h2>Generating Predictions and Class Probabilities</h2>\n",
    "\n",
    "Once the models are trained, we generate predictions and class probabilities on the testing set using the `.predict()` and `.predict_proba()` methods, respectively. These methods mirror the functionality of `sklearn` and `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cad2054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for each model...\n",
      "Predicting with dummy...\n",
      "Predicting with tree...\n",
      "Predicting with forest...\n",
      "Predicting with support_vector...\n",
      "Predicting with neural_network...\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Predicting with logistic...\n",
      "Predicting with gaussian_NB...\n",
      "Predicting with bernoulli_NB...\n",
      "Predicting with adaboost...\n",
      "Predictions on training set is saved!\n",
      "Generating probabilities for each model...\n",
      "Computing dummy survival probabilities...\n",
      "Computing tree survival probabilities...\n",
      "Computing forest survival probabilities...\n",
      "Computing support_vector survival probabilities...\n",
      "Computing neural_network survival probabilities...\n",
      "20/20 [==============================] - 0s 948us/step\n",
      "Computing logistic survival probabilities...\n",
      "Computing gaussian_NB survival probabilities...\n",
      "Computing bernoulli_NB survival probabilities...\n",
      "Computing adaboost survival probabilities...\n",
      "All probabilities are computed!\n",
      "Probabilities on training set is saved!\n",
      "Generating predictions for each model...\n",
      "Predicting with dummy...\n",
      "Predicting with tree...\n",
      "Predicting with forest...\n",
      "Predicting with support_vector...\n",
      "Predicting with neural_network...\n",
      "9/9 [==============================] - 0s 864us/step\n",
      "Predicting with logistic...\n",
      "Predicting with gaussian_NB...\n",
      "Predicting with bernoulli_NB...\n",
      "Predicting with adaboost...\n",
      "Predictions on testing set is saved!\n",
      "Generating probabilities for each model...\n",
      "Computing dummy survival probabilities...\n",
      "Computing tree survival probabilities...\n",
      "Computing forest survival probabilities...\n",
      "Computing support_vector survival probabilities...\n",
      "Computing neural_network survival probabilities...\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "Computing logistic survival probabilities...\n",
      "Computing gaussian_NB survival probabilities...\n",
      "Computing bernoulli_NB survival probabilities...\n",
      "Computing adaboost survival probabilities...\n",
      "All probabilities are computed!\n",
      "Probabilities on testing set is saved!\n",
      "All predictions and probabilities are saved!\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions and probabilities for each model\n",
    "\n",
    "data: list = [(X_train, y_train, id_train, 'train'), (X_test, y_test, id_test, 'test')]\n",
    "    \n",
    "for X, y, id_, name in data:\n",
    "    \n",
    "    # Generate predictions for each model\n",
    "    predictions_df: pd.DataFrame = Titanic_ML.predict(X)\n",
    "    predictions_df['y_true'] = y\n",
    "    predictions_df['PassengerId'] = id_.values\n",
    "    predictions_df = predictions_df[['PassengerId', 'y_true'] + [model for model in Titanic_ML.models_.keys()]]\n",
    "    \n",
    "    # Save the predictions\n",
    "    predictions_df.to_csv(f'csv/data/{name}_predictions.csv', index = False)\n",
    "    \n",
    "    print(f'Predictions on {name}ing set is saved!')\n",
    "    \n",
    "    # Generate probabilities for each model\n",
    "    probabilities_df: pd.DataFrame = Titanic_ML.predict_proba(X)\n",
    "    probabilities_df['PassengerId'] = id_.values\n",
    "    probabilities_df = probabilities_df[['PassengerId'] + [model for model in Titanic_ML.models_.keys()]]\n",
    "    \n",
    "    # Save the probabilities\n",
    "    probabilities_df.to_csv(f'csv/data/{name}_survival_rate.csv', index = False)\n",
    "    \n",
    "    print(f'Probabilities on {name}ing set is saved!')\n",
    "\n",
    "print('All predictions and probabilities are saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93487e",
   "metadata": {},
   "source": [
    "Finally, we save the predictions and probabilities into the `csv/data/train_predictions.csv`, `csv/data/test_predictions.csv`, `csv/data/train_predictions.csv`, and `csv/data/test_predictions.csv` files respectively, marking the completion of our model training stage. The next step involves evaluating the performance of each model. Please refer to the `demo_model_evaluation.ipynb` file for more information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testtest",
   "language": "python",
   "name": "testtest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
